{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码实现了搭建了网络训练的基本框架\n",
    "# 在此代码的基础上，需要实现下面的内容（请留意TODO）：\n",
    "# 1、实现全连接神经网络的前向传播和反向传播（不使用torch.nn搭建网络，不使用backward方法进行反传）\n",
    "# 2、实现交叉熵损失函数(不使用torch.nn.CrossEntropyLoss)\n",
    "# 3、实现带动量的SGD优化器（不使用torch.optim.SGD）\n",
    "# 代码可根据自己需要修改，实现上述内容即可\n",
    "#  提示：\n",
    "# 在实现过程中，可使用xxx.shape观察网络和数据的维度\n",
    "# 可以将自己实现的输出与pytorch函数的输出进行比较(如损失函数与优化器)，观察自己的模块是否正常工作\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    y = x.clone()\n",
    "    y[y<0] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(tensor1, tensor2):\n",
    "    dim = tensor1.dim() - 1\n",
    "    return - torch.sum(tensor1 * torch.log(tensor2 + 1e-10), dim=dim)\n",
    "\n",
    "def softmax(tensor, dim=None):\n",
    "    if dim is None:\n",
    "        dim = tensor.dim() - 1\n",
    "        \n",
    "    max_values, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
    "    e = torch.exp(tensor-max_values)\n",
    "    return e / torch.sum(e, dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：在这里你需要实现一些类来实现上述三个内容\n",
    "# 类的设计并无具体要求，能实现所需功能即可\n",
    "# 比如，可以考虑先构建单层全连接层Layer类，再组成整体网络Net类\n",
    "# 可单独设置Loss类与SGD类，也可以将这些功能的实现放到Net类中\n",
    "\n",
    "# 一种可能的类的设计为\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):  # TODO:在这里实现全连接神经网络\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, initial=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.w1 = torch.randn((input_size, hidden_size1), dtype=torch.float64)\n",
    "        self.b1 = torch.randn((hidden_size1,), dtype=torch.float64)\n",
    "        self.w2 = torch.randn((hidden_size1, hidden_size2), dtype=torch.float64)\n",
    "        self.b2 = torch.randn((hidden_size2,), dtype=torch.float64)\n",
    "        self.w3 = torch.randn((hidden_size2, output_size), dtype=torch.float64)\n",
    "        self.b3 = torch.randn((output_size,), dtype=torch.float64)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.w1)\n",
    "        nn.init.kaiming_normal_(self.w2)\n",
    "        nn.init.kaiming_normal_(self.w3)\n",
    "        \n",
    "        # 梯度\n",
    "        self.w1_grad = torch.zeros_like(self.w1, dtype=torch.float64)\n",
    "        self.b1_grad = torch.zeros_like(self.b1, dtype=torch.float64)\n",
    "        self.w2_grad = torch.zeros_like(self.w2, dtype=torch.float64)\n",
    "        self.b2_grad = torch.zeros_like(self.b2, dtype=torch.float64)\n",
    "        self.w3_grad = torch.zeros_like(self.w3, dtype=torch.float64)\n",
    "        self.b3_grad = torch.zeros_like(self.b3, dtype=torch.float64)\n",
    "        \n",
    "        # 前一次迭代的梯度\n",
    "        self.w1_t = torch.zeros_like(self.w1, dtype=torch.float64)\n",
    "        self.b1_t = torch.zeros_like(self.b1, dtype=torch.float64)\n",
    "        self.w2_t = torch.zeros_like(self.w2, dtype=torch.float64)\n",
    "        self.b2_t = torch.zeros_like(self.b2, dtype=torch.float64)\n",
    "        self.w3_t = torch.zeros_like(self.w3, dtype=torch.float64)\n",
    "        self.b3_t = torch.zeros_like(self.b3, dtype=torch.float64)        \n",
    "        \n",
    "    def __call__(self, data):\n",
    "        if torch.is_tensor(data):\n",
    "            self.input = data.double()\n",
    "        else:\n",
    "            self.input = torch.tensor(data, dtype=torch.float64)\n",
    "            \n",
    "        self.h1 = torch.mm(self.input, self.w1) + self.b1\n",
    "        self.h1_relu = relu(self.h1)\n",
    "        \n",
    "        self.h2 = torch.mm(self.h1_relu, self.w2) + self.b2\n",
    "        self.h2_relu = relu(self.h2)\n",
    "        \n",
    "        self.output = torch.mm(self.h2_relu, self.w3) + self.b3\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def loss(self, label):\n",
    "        if not torch.is_tensor(label):\n",
    "            self.label = torch.tensor(label)\n",
    "        else:\n",
    "            self.label = label\n",
    "            \n",
    "        catogery = self.output.shape[1]\n",
    "        onehot = torch.zeros((label.shape[0], catogery), dtype=torch.float64)\n",
    "        self.onehot_label = onehot.scatter(1, label.unsqueeze(1), 1.0)\n",
    "        \n",
    "        loss = crossEntropy(self.onehot_label, softmax(self.output))\n",
    "\n",
    "        return torch.mean(loss)     \n",
    "    \n",
    "    def backward(self):\n",
    "        # 记录上一次迭代的梯度\n",
    "        self.w1_t, self.b1_t = self.w1_grad, self.b1_grad\n",
    "        self.w2_t, self.b2_t = self.w2_grad, self.b2_grad\n",
    "        self.w3_t, self.b3_t = self.w3_grad, self.b3_grad\n",
    "        \n",
    "        output_grad = (softmax(self.output)  - self.onehot_label) / self.output.shape[0]\n",
    "        \n",
    "        \n",
    "        self.w3_grad = torch.mm(self.h2_relu.permute(1, 0), output_grad)\n",
    "        self.b3_grad = torch.sum(output_grad, dim=0)\n",
    "        \n",
    "        h2_relu_grad = torch.mm(output_grad, self.w3.permute(1, 0))\n",
    "        h2_grad = h2_relu_grad.clone()\n",
    "        h2_grad[self.h2 < 0] = 0\n",
    "        \n",
    "        self.w2_grad = torch.mm(self.h1_relu.permute(1, 0), h2_grad)\n",
    "        self.b2_grad = torch.sum(h2_grad, dim=0)\n",
    "        \n",
    "        h1_relu_grad = torch.mm(h2_grad, self.w2.permute(1, 0))\n",
    "        h1_grad = h1_relu_grad.clone()\n",
    "        h1_grad[self.h1 < 0] = 0\n",
    "        \n",
    "        self.w1_grad = torch.mm(self.input.permute(1, 0), h1_grad)\n",
    "        self.b1_grad = torch.sum(h1_grad, dim=0)\n",
    "        \n",
    "        \n",
    "    def SGD(self, lrate, gama):\n",
    "        self.w1 -= gama * lrate * self.w1_t + lrate * self.w1_grad\n",
    "        self.b1 -= gama * lrate * self.b1_t +lrate * self.b1_grad\n",
    "        self.w2 -= gama *  lrate * self.w2_t + lrate * self.w2_grad\n",
    "        self.b2 -= gama * lrate * self.b2_t + lrate * self.b2_grad\n",
    "        self.w3 -= gama *  lrate * self.w3_t + lrate * self.w3_grad\n",
    "        self.b3 -= gama * lrate * self.b3_t + lrate * self.b3_grad\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对训练过程的准确率和损失画图\n",
    "def training_process(train_loss, train_acc, test_acc):\n",
    "    shape = train_loss.shape[0]\n",
    "    epoch = np.arange(1, shape+1)\n",
    "\n",
    "    plt.plot(epoch, test_acc, label=\"testAcc\")\n",
    "    plt.plot(epoch, train_acc, label=\"trainAcc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"accuracy on train set and test set\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch, train_loss, label=\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"loss on train set\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 可直接使用这组超参数进行训练，也可以自己尝试调整\n",
    "    lr = 0.02  # 学习率\n",
    "    epoch = 20  # 迭代次数\n",
    "    batch_size = 128  # 每一批次的大小\n",
    "    gama = 0.9\n",
    "    \n",
    "    # 对数据集图片做标准化并转为tensor\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])  # 对训练集的transform\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])  # 对测试集的transform\n",
    "\n",
    "    # 借助torchvision中的函数读取MNIST，请将参数root换为自己数据存放的路径，或者设置download=True下载数据集\n",
    "    # 读MNIST训练集\n",
    "    path = \"G:\\\\grade3_second\\\\artificial neural networks\\\\lab\\Lab_weeks 3-4\\\\week3\"\n",
    "    trainSet = MNIST(root=path, train=True, transform=transform_train, download=False)\n",
    "    trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    # 读MNIST测试集\n",
    "    testSet = MNIST(root=path, train=False, transform=transform_test, download=False)\n",
    "    testLoader = torch.utils.data.DataLoader(testSet, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch 99 index: acc: 61.570312 loss:1.824845\n",
      "0 epoch 199 index: acc: 72.082031 loss:1.188068\n",
      "0 epoch 299 index: acc: 76.966146 loss:0.936535\n",
      "0 epoch 399 index: acc: 79.750000 loss:0.796455\n",
      "------- 0-------\n",
      "Epoch loss: 340.74\n",
      "Train acc: 81.11%\n",
      "Test acc: 0.00%\n",
      "\n",
      "1 epoch 99 index: acc: 90.734375 loss:0.299952\n",
      "1 epoch 199 index: acc: 90.964844 loss:0.295352\n",
      "1 epoch 299 index: acc: 91.348958 loss:0.285275\n",
      "1 epoch 399 index: acc: 91.679688 loss:0.276909\n",
      "------- 1-------\n",
      "Epoch loss: 126.38\n",
      "Train acc: 91.75%\n",
      "Test acc: 0.00%\n",
      "\n",
      "2 epoch 99 index: acc: 93.335938 loss:0.223295\n",
      "2 epoch 199 index: acc: 93.445312 loss:0.218423\n",
      "2 epoch 299 index: acc: 93.770833 loss:0.208506\n",
      "2 epoch 399 index: acc: 93.755859 loss:0.207511\n",
      "------- 2-------\n",
      "Epoch loss: 96.39\n",
      "Train acc: 93.63%\n",
      "Test acc: 0.00%\n",
      "\n",
      "3 epoch 99 index: acc: 94.664062 loss:0.180838\n",
      "3 epoch 199 index: acc: 94.601562 loss:0.181904\n",
      "3 epoch 299 index: acc: 94.627604 loss:0.180526\n",
      "3 epoch 399 index: acc: 94.654297 loss:0.177265\n",
      "------- 3-------\n",
      "Epoch loss: 81.60\n",
      "Train acc: 94.59%\n",
      "Test acc: 0.00%\n",
      "\n",
      "4 epoch 99 index: acc: 95.179688 loss:0.156038\n",
      "4 epoch 199 index: acc: 95.445312 loss:0.150023\n",
      "4 epoch 299 index: acc: 95.312500 loss:0.153704\n",
      "4 epoch 399 index: acc: 95.287109 loss:0.152651\n",
      "------- 4-------\n",
      "Epoch loss: 71.35\n",
      "Train acc: 95.16%\n",
      "Test acc: 0.00%\n",
      "\n",
      "5 epoch 99 index: acc: 95.875000 loss:0.135186\n",
      "5 epoch 199 index: acc: 95.953125 loss:0.132688\n",
      "5 epoch 299 index: acc: 95.815104 loss:0.137104\n",
      "5 epoch 399 index: acc: 95.787109 loss:0.137007\n",
      "------- 5-------\n",
      "Epoch loss: 63.84\n",
      "Train acc: 95.66%\n",
      "Test acc: 0.00%\n",
      "\n",
      "6 epoch 99 index: acc: 96.273438 loss:0.119517\n",
      "6 epoch 199 index: acc: 96.273438 loss:0.121667\n",
      "6 epoch 299 index: acc: 96.247396 loss:0.123089\n",
      "6 epoch 399 index: acc: 96.171875 loss:0.124884\n",
      "------- 6-------\n",
      "Epoch loss: 57.58\n",
      "Train acc: 96.07%\n",
      "Test acc: 0.00%\n",
      "\n",
      "7 epoch 99 index: acc: 96.843750 loss:0.106295\n",
      "7 epoch 199 index: acc: 96.570312 loss:0.113617\n",
      "7 epoch 299 index: acc: 96.609375 loss:0.111137\n",
      "7 epoch 399 index: acc: 96.556641 loss:0.113890\n",
      "------- 7-------\n",
      "Epoch loss: 52.72\n",
      "Train acc: 96.41%\n",
      "Test acc: 0.00%\n",
      "\n",
      "8 epoch 99 index: acc: 96.992188 loss:0.096617\n",
      "8 epoch 199 index: acc: 96.792969 loss:0.104281\n",
      "8 epoch 299 index: acc: 96.815104 loss:0.103750\n",
      "8 epoch 399 index: acc: 96.781250 loss:0.104079\n",
      "------- 8-------\n",
      "Epoch loss: 48.63\n",
      "Train acc: 96.64%\n",
      "Test acc: 0.00%\n",
      "\n",
      "9 epoch 99 index: acc: 96.984375 loss:0.102041\n",
      "9 epoch 199 index: acc: 96.933594 loss:0.100188\n",
      "9 epoch 299 index: acc: 97.020833 loss:0.097799\n",
      "9 epoch 399 index: acc: 97.062500 loss:0.096215\n",
      "------- 9-------\n",
      "Epoch loss: 45.29\n",
      "Train acc: 96.89%\n",
      "Test acc: 0.00%\n",
      "\n",
      "10 epoch 99 index: acc: 97.421875 loss:0.084692\n",
      "10 epoch 199 index: acc: 97.265625 loss:0.088318\n",
      "10 epoch 299 index: acc: 97.203125 loss:0.090380\n",
      "10 epoch 399 index: acc: 97.205078 loss:0.090135\n",
      "-------10-------\n",
      "Epoch loss: 42.45\n",
      "Train acc: 97.02%\n",
      "Test acc: 0.00%\n",
      "\n",
      "11 epoch 99 index: acc: 97.421875 loss:0.083151\n",
      "11 epoch 199 index: acc: 97.492188 loss:0.080671\n",
      "11 epoch 299 index: acc: 97.619792 loss:0.079770\n",
      "11 epoch 399 index: acc: 97.515625 loss:0.080597\n",
      "-------11-------\n",
      "Epoch loss: 38.95\n",
      "Train acc: 97.26%\n",
      "Test acc: 0.00%\n",
      "\n",
      "12 epoch 99 index: acc: 97.578125 loss:0.077906\n",
      "12 epoch 199 index: acc: 97.578125 loss:0.076275\n",
      "12 epoch 299 index: acc: 97.598958 loss:0.077309\n",
      "12 epoch 399 index: acc: 97.562500 loss:0.078887\n",
      "-------12-------\n",
      "Epoch loss: 37.05\n",
      "Train acc: 97.39%\n",
      "Test acc: 0.00%\n",
      "\n",
      "13 epoch 99 index: acc: 97.976562 loss:0.069716\n",
      "13 epoch 199 index: acc: 97.957031 loss:0.069156\n",
      "13 epoch 299 index: acc: 97.861979 loss:0.070830\n",
      "13 epoch 399 index: acc: 97.773438 loss:0.073433\n",
      "-------13-------\n",
      "Epoch loss: 35.15\n",
      "Train acc: 97.56%\n",
      "Test acc: 0.00%\n",
      "\n",
      "14 epoch 99 index: acc: 98.000000 loss:0.071346\n",
      "14 epoch 199 index: acc: 98.042969 loss:0.067834\n",
      "14 epoch 299 index: acc: 97.953125 loss:0.067759\n",
      "14 epoch 399 index: acc: 97.878906 loss:0.069634\n",
      "-------14-------\n",
      "Epoch loss: 33.18\n",
      "Train acc: 97.69%\n",
      "Test acc: 0.00%\n",
      "\n",
      "15 epoch 99 index: acc: 98.039062 loss:0.063155\n",
      "15 epoch 199 index: acc: 98.121094 loss:0.063346\n",
      "15 epoch 299 index: acc: 97.994792 loss:0.065474\n",
      "15 epoch 399 index: acc: 97.966797 loss:0.066062\n",
      "-------15-------\n",
      "Epoch loss: 31.04\n",
      "Train acc: 97.80%\n",
      "Test acc: 0.00%\n",
      "\n",
      "16 epoch 99 index: acc: 97.953125 loss:0.064450\n",
      "16 epoch 199 index: acc: 98.046875 loss:0.063525\n",
      "16 epoch 299 index: acc: 98.072917 loss:0.062709\n",
      "16 epoch 399 index: acc: 98.041016 loss:0.062887\n",
      "-------16-------\n",
      "Epoch loss: 29.79\n",
      "Train acc: 97.88%\n",
      "Test acc: 0.00%\n",
      "\n",
      "17 epoch 99 index: acc: 98.156250 loss:0.060177\n",
      "17 epoch 199 index: acc: 98.210938 loss:0.058081\n",
      "17 epoch 299 index: acc: 98.197917 loss:0.058254\n",
      "17 epoch 399 index: acc: 98.199219 loss:0.058715\n",
      "-------17-------\n",
      "Epoch loss: 27.75\n",
      "Train acc: 98.04%\n",
      "Test acc: 0.00%\n",
      "\n",
      "18 epoch 99 index: acc: 98.335938 loss:0.056259\n",
      "18 epoch 199 index: acc: 98.324219 loss:0.056875\n",
      "18 epoch 299 index: acc: 98.278646 loss:0.057380\n",
      "18 epoch 399 index: acc: 98.251953 loss:0.057719\n",
      "-------18-------\n",
      "Epoch loss: 26.86\n",
      "Train acc: 98.11%\n",
      "Test acc: 0.00%\n",
      "\n",
      "19 epoch 99 index: acc: 98.523438 loss:0.050614\n",
      "19 epoch 199 index: acc: 98.492188 loss:0.051658\n",
      "19 epoch 299 index: acc: 98.531250 loss:0.050151\n",
      "19 epoch 399 index: acc: 98.416016 loss:0.053297\n",
      "-------19-------\n",
      "Epoch loss: 25.16\n",
      "Train acc: 98.24%\n",
      "Test acc: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # 训练数据的记录\n",
    "    train_acc = np.zeros(epoch)\n",
    "    test_acc = np.zeros(epoch)\n",
    "    train_loss = np.zeros(epoch)\n",
    "    \n",
    "    # TODO：在这里对你实现的类进行实例化，之后开始对模型进行训练\n",
    "    input_size = 784\n",
    "    hidden_size1 = 128\n",
    "    hidden_size2 = 128\n",
    "    output_size = 10\n",
    "    net = Net(input_size, hidden_size1, hidden_size2, output_size)  # 具体的实例化根据你的实现而定，此处只做示意（包括下面两行）\n",
    " \n",
    "    # 重复训练epoch次\n",
    "    for epo in range(epoch):\n",
    "        epoch_loss = 0  # 当前epoch的损失\n",
    "        correct1 = 0  # 当前epoch的训练集准确率\n",
    "        correct2 = 0  # 当前epoch的测试集准确率\n",
    "\n",
    "        # 训练阶段\n",
    "        # 利用每个mini-batch对网络进行更新\n",
    "        for index, (data, label) in enumerate(trainLoader):  # 从trainLoader读取一个mini-batch\n",
    "            \n",
    "            # index是当前mini-batch的序号，data是图像，label是标签，data和label都有batch_size个\n",
    "            data = data.view(data.size(0), -1)  # 展开，将输入的维度从[batch_size, 1, 28, 28]变成[batch_size, 784]\n",
    "            output = net(data)  # TODO：完成前向传播，其中net是你实现的三层全连接神经网络，具体调用形式根据你的实现而定（包括下面三个）\n",
    "\n",
    "            # 计算训练集准确率，output是网络的输出，维度应为[batch_size, 10]\n",
    "            _, prediction = torch.max(output, 1)\n",
    "            correct1 += (prediction == label).sum()\n",
    "            loss = net.loss(label)\n",
    "            net.backward()\n",
    "            net.SGD(lr, gama)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if index % 100 == 99:\n",
    "                print('%d epoch %d index: acc: %f loss:%f' % (epo, index, correct1.item() *100 / ((index + 1) * batch_size), epoch_loss / (index + 1)))\n",
    "\n",
    "        # 测试阶段\n",
    "        # 测试时不需要tensor的梯度，可调用no_grad关掉梯度\n",
    "#         with torch.no_grad():\n",
    "#             for index, (data, label) in enumerate(testLoader):# 从testLoader读取一个mini-batch\n",
    "#                 data = data.view(data.size(0), -1)\n",
    "#                 output = net(data)  # 与上面对前向传播的实现保持一致\n",
    "\n",
    "#                 # 计算测试集准确率\n",
    "#                 _, prediction = torch.max(output.data, 1)\n",
    "#                 correct2 += (prediction == label).sum()\n",
    "\n",
    "        # 计算训练集和测试集准确率\n",
    "        epoch_train_acc = (int(correct1) * 100 / 60000)\n",
    "        epoch_test_acc = (int(correct2) * 100 / 10000)\n",
    "\n",
    "        # 输出当前epoch的信息\n",
    "        print(\"-------%2d-------\" % epo)\n",
    "        print(\"Epoch loss: %4.2f\" % epoch_loss)\n",
    "        print(\"Train acc: %3.2f%%\" % epoch_train_acc)\n",
    "        print(\"Test acc: %3.2f%%\" % epoch_test_acc)\n",
    "        print()\n",
    "\n",
    "        # 记录loss和accuracy\n",
    "        train_acc[epo] = epoch_train_acc\n",
    "        test_acc[epo] = epoch_test_acc\n",
    "        train_loss[epo] = epoch_loss\n",
    "\n",
    "        # 至此当前epoch结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 96.23%\n"
     ]
    }
   ],
   "source": [
    "#         测试阶段\n",
    "#         测试时不需要tensor的梯度，可调用no_grad关掉梯度\n",
    "with torch.no_grad():\n",
    "    for index, (data, label) in enumerate(testLoader):# 从testLoader读取一个mini-batch\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = net(data)  # 与上面对前向传播的实现保持一致\n",
    "\n",
    "        # 计算测试集准确率\n",
    "        _, prediction = torch.max(output.data, 1)\n",
    "        correct2 += (prediction == label).sum()\n",
    "epoch_test_acc = (int(correct2) * 100 / 10000)\n",
    "print(\"Test acc: %3.2f%%\" % epoch_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
